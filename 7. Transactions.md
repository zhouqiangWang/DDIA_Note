# 7. Transactions

## 1. Concept of Transactions

Transactions are an abstraction layer that allows an application to pretend that certain concurrency problems and certain kinds of hardware and software faults don’t exist. A large class of errors is reduced down to a simple transaction abort, and the application just needs to try again.

In the harsh reality of data systems, many things can go wrong:
* The database software or hardware may fail at any time (including in the middle of a write operation).
* The application may crash at any time (including halfway through a series of operations).
* Interruptions in the network can unexpectedly cut off the application from the database, or one database node from another.
* Several clients may write to the database at the same time, overwriting each other’s changes.
* A client may read data that doesn’t make sense because it has only partially been updated.
* Race conditions between clients can cause surprising bugs.


With the hype around this new crop of distributed databases, there emerged a popular belief that transactions were the antithesis of scalability, and that any large-scale system would have to abandon transactions in order to maintain good performance and high availability. On the other hand, transactional guarantees are some‐ times presented by database vendors as an essential requirement for “**serious applications**” with “**valuable data**”. Both viewpoints are **pure hyperbole**.

### The Meaning of ACID
which stands for *Atomicity, Consistency, Isolation, and Durability*.

However, in practice, one database’s implementation of ACID does not equal another’s implementation. For example, as we shall see, there is a lot of ambiguity around the meaning of isolation [8]. **The high-level idea is sound, but the devil is in the details.** Today, when a system claims to be “ACID compliant,” it’s unclear what guarantees you can actually expect. **ACID has unfortunately become mostly a marketing term.**

#### Atomicity

#### consistency

The word consistency is terribly overloaded:
*  In Chapter 5 we discussed replica consistency and the issue of eventual consis‐ tency that arises in asynchronously replicated systems (see “Problems with Repli‐ cation Lag” on page 161).
*  Consistent hashing is an approach to partitioning that some systems use for reba‐ lancing (see “Consistent Hashing” on page 204).
* In the CAP theorem (see Chapter 9), the word consistency is used to mean linearizability (see “Linearizability” on page 324).
* In the context of ACID, consistency refers to an application-specific notion of the database being in a “good state.”

Atomicity, isolation, and durability are properties of the database, whereas consistency is a property of the application. 

#### Isolation

Isolation means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes.

![](https://i.imgur.com/bAFy6SL.png)

The classic database textbooks formalize isolation as serializability, which means that each transaction can pretend that it is the only transaction running on the entire database. The database ensures that when the transactions have committed, the result is the same as if they had run serially (one after another), even though in reality they may have run con‐ currently 

#### Durability
In a single-node database, durability typically means that the data has been written to nonvolatile storage such as a hard drive or SSD.

#### Single-Object and Multi-Object Operations

![](https://i.imgur.com/yNkFpmc.png)

![](https://i.imgur.com/XnOYfTD.png)

#### Handling errors and aborts
A key feature of a transaction is that it can be aborted and safely retried if an error occurred. 

## 3. Serializability

### Actual Serial Execution

The simplest way of avoiding concurrency problems is to remove the concurrency entirely: to execute only one transaction at a time, in serial order, on a single thread.

Two developments caused this rethink:
* RAM became cheap enough that for many use cases is now feasible to keep the entire active dataset in memory. When all data that a transaction needs to access is in memory, transactions can execute much faster than if they have to wait for data to be loaded from disk.
* Database designers realized that OLTP transactions are usually short and only make a small number of reads and writes. By contrast, long-running analytic queries are typically read- only, so they can be run on a consistent snapshot (using snapshot isolation) outside of the serial execution loop.


#### Encapsulating transactions in stored procedures
![](https://i.imgur.com/4qaS7TF.png)

### Two-Phase Locking (2PL)

*2PL is not 2PC*
Note that while two-phase locking (2PL) sounds very similar to two-phase commit (2PC), they are completely different things.

The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. The lock is used as follows:
* If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simulta‐ neously, but if another transaction already has an exclusive lock on the object, these transactions must wait.
* If a transaction wants to write to an object, it must first acquire the lock in exclu‐ sive mode. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait.
* If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly.
* After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two- phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released.

### Performance of two-phase locking
The big downside of two-phase locking, and the reason why it hasn’t been used by everybody since the 1970s, is performance: transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation.

### Predicate locks
A predicate lock restricts access as follows:
* If transaction A wants to read objects matching some condition, like in that SELECT query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query.
* If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue.
The key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms)

### Index-range locks

## Serializable Snapshot Isolation (SSI)

#### Pessimistic versus optimistic concurrency control
Two-phase locking is a so-called pessimistic concurrency control mechanism: it is based on the principle that if anything might possibly go wrong (as indicated by a lock held by another transaction), it’s better to wait until the situation is safe again before doing anything. It is like mutual exclusion, which is used to protect data struc‐ tures in multi-threaded programming.

#### Decisions based on an outdated premise
How does the database know if a query result might have changed? There are two cases to consider:
• Detecting reads of a stale MVCC object version (uncommitted write occurred before the read)
• Detecting writes that affect prior reads (the write occurs after the read)


#### Detecting stale MVCC reads
![](https://i.imgur.com/ZaDtQMC.png)

#### Detecting writes that affect prior reads
![](https://i.imgur.com/vZYigiR.png)
when transaction 43 wants to commit, the conflicting write from 42 has already been committed, so 43 must abort.

##### Performance of serializable snapshot isolation

one trade-off is the granularity at which transactions’ reads and writes are tracked. If the database keeps track of each transaction’s activity in great detail, **it can be precise about which transactions need to abort, but the bookkeeping overhead can become significant. Less detailed tracking is faster, but may lead to more transac‐ tions being aborted than strictly necessary.**

* *Dirty reads*
One client reads another client’s writes before they have been committed. The read committed isolation level and stronger levels prevent dirty reads.
* *Dirty writes*
One client overwrites data that another client has written, but not yet committed. Almost all transaction implementations prevent dirty writes.
* *Read skew (nonrepeatable reads)*
A client sees different parts of the database at different points in time. This issue is most commonly prevented with snapshot isolation, which allows a transaction to read from a consistent snapshot at one point in time. It is usually implemented with multi-version concurrency control (MVCC).
* *Lost updates*
Two clients concurrently perform a read-modify-write cycle. One overwrites the other’s write without incorporating its changes, so data is lost. Some implemen‐ tations of snapshot isolation prevent this anomaly automatically, while others require a manual lock (SELECT FOR UPDATE).
* *Write skew*
A transaction reads something, makes a decision based on the value it saw, and writes the decision to the database. However, by the time the write is made, the premise of the decision is no longer true. Only serializable isolation prevents this anomaly.
* *Phantom reads*
A transaction reads objects that match some search condition. Another client makes a write that affects the results of that search. Snapshot isolation prevents straightforward phantom reads, but phantoms in the context of write skew require special treatment, such as index-range locks.

Weak isolation levels protect against some of those anomalies but leave you, the application developer, to handle others manually (e.g., using explicit locking). Only serializable isolation protects against all of these issues. We discussed three different approaches to implementing serializable transactions:

